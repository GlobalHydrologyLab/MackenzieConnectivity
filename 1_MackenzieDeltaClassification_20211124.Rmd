---
title: "1_ColvilleDeltaConnectivityAnalysis_classGeneration"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Libraries
```{r}
library(tidyverse)
library(sf)
library(lubridate)
library(grDevices)
library(mapview)
library(extrafont)
library(ggpubr)
library(ggmap)
library(RgoogleMaps)
library(broom)
library(HistDAWass)
library(tidyhydat)
library(sp)
library(data.table)
#Import libraries for rf and dt
library(caret) 
library(e1071)
library(Boruta)
library(tidymodels)
library(skimr)
```

# Functions
```{r}
#takes RGB to calculate dominant wavelength
chroma <- function(R, G, B) {  
  require(colorscience)

# Convert R,G, and B spectral reflectance to dominant wavelength #based
# on CIE chromaticity color space

# see Wang et al 2015. MODIS-Based Radiometric Color Extraction and
# Classification of Inland Water With the Forel-Ule
# Scale: A Case Study of Lake Taihu

# chromaticity.diagram.color.fill()
Xi <- 2.7689*R + 1.7517*G + 1.1302*B
Yi <- 1.0000*R + 4.5907*G + 0.0601*B
Zi <- 0.0565*G + 5.5943*B

# calculate coordinates on chromaticity diagram
x <-  Xi / (Xi + Yi +  Zi)
y <-  Yi / (Xi + Yi +  Zi)
z <-  Zi / (Xi + Yi +  Zi)

# calculate hue angle
alpha <- atan2( (x - (1/3)), (y - (1/3))) * 180/pi

# make look up table for hue angle to wavelength conversion
cie <- cccie31 %>%
  dplyr::mutate(a = atan2( (x - (1/3)), (y - (1/3))) * 180/pi) %>%
  dplyr::filter(wlnm <= 700) %>%
  dplyr::filter(wlnm >=380) 

# find nearest dominant wavelength to hue angle
wl <- cie[as.vector(sapply(alpha,function(x) which.min(abs(x - cie$a)))) , 'wlnm']

return(wl)
}
```

# Imports and file paths
```{r}
# Names of files and folders for reflectance data
import.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/GEE Downloads"
import.Lakes = "MackenzieLakeExport_20210914.csv"
import.channels = "MackenzieChannelExport_20220525.csv"
# 
# #Name of file and folder for classification export
# export.filePath = "E:/Research/DeltaicConnectivity/ColvilleDelta/Data/Outputs"
# export.fileName.dt = "colville_dt_20211022"
# export.fileName.rf = "colville_rf_20211022"
# export.fileName.pct = "colville_pct_20211022"
# export.fileName.km = "colville_km_20211022"
# 
# 
# 
#Name of file and folder for lake shapefiles & island polygon shapefiles
shapeFiles.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/shapeFiles"
lakes.shapeFile = "mackenzieGoodLakes.shp"
islands.shapeFile = "vectorIslandArea2.shp"
setwd(shapeFiles.filePath)
lakes.sf = st_read(lakes.shapeFile)
islands.sf=st_read(islands.shapeFile)

# 
# # Name of file and folder for GECI validation data
valFile.path="C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/GEE Downloads/trainingData"
valFileName = "trainingData_1001lakes.shp"
# 
# # Name of file and folder for figures
# figures.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Papers/ColvilleDeltaConnectivity/Figures/Figures_raw"
# dens.figure.name = "densityPlotExample.pdf"
# tree.figure.name = "DecisionTreeFigure.pdf"

```

# Rank the discharge years during 2003-2019 period
```{r}
Mack_at_redRiver_num ="10LC014"
Mack_at_ftSimp_num ="10GC001"
mack_rr=hy_daily_flows(Mack_at_redRiver_num) %>% mutate(
  year=year(Date),
  month = month(Date),
  doy = yday(Date)) %>%
  filter(year>=2000 & year<=2019) %>% filter(month>=5 & month<=9) %>% 
  filter(Parameter=="Flow") %>% group_by(year) %>% 
  summarise(mean_discharge=mean(Value),
            total_discharge=sum(Value), 
            max_discharge = max(Value),
            max_dis_doy=doy[which.max(Value)]) %>% ungroup() %>% 
  arrange(max_discharge) %>% 
  mutate(dischargeGroup = c(rep(1,5), rep(2,5), rep(3,5), rep(4,5))) %>% 
  mutate(site = Mack_at_redRiver_num)
mack_rr

mack_fs=hy_daily_flows(Mack_at_ftSimp_num) %>% mutate(
  year=year(Date),
  month = month(Date),
  doy = yday(Date)) %>%
  filter(year>=2000 & year<=2020) %>% filter(month>=5 & month<=9) %>% 
  filter(Parameter=="Flow") %>% group_by(year) %>% 
  summarise(mean_discharge=mean(Value),
            total_discharge=sum(Value), 
            max_discharge = max(Value),
            max_dis_doy=doy[which.max(Value)]) %>% ungroup() %>% 
  arrange(max_discharge) %>% 
  mutate(dischargeGroup = c(rep(1,5), rep(2,5), rep(3,5), rep(4,5))) %>% 
  mutate(site = Mack_at_ftSimp_num)
mack_fs
highest.maxDis.yrs = mack_fs$year[mack_fs$dischargeGroup==4]
```

# Import and filter the lake and channel data  - calculates both for mean, tenth percentile and 90% dom wv values.
```{r}
setwd(import.filePath)
#import lake data
all.lakes = read.csv(import.Lakes) %>% dplyr::as_tibble()
#import all lake buffer/channel file
all.channels = read.csv(import.channels) %>% dplyr::as_tibble()


# Filter lake and channel data
lakes.maxPix = all.lakes %>% group_by(OBJECTID) %>% summarise(max.pix = max(Red_count))# count max number of pix--> likely the clear sky total of the lake
lakes.filter = all.lakes %>% 
  left_join(lakes.maxPix, by="OBJECTID") %>% 
  mutate(dateTime = as_datetime(system.time_start_mean*0.001),
         date = as_date(dateTime)) %>% 
  select(-system.time_start_mean, -count) %>% 
  filter(Red_count/max.pix >= 0.5)  # each observation must be made up of at least 50% of the lake area
lakes.combo=lakes.filter ############temporary until we can figure out the color code
channels.maxPix = all.channels %>% group_by(fid) %>% summarise(max.pix = max(Red_count))
channels.filter = all.channels %>% 
  left_join(channels.maxPix, by="fid") %>% 
  mutate(dateTime = as_datetime(`system.time_start_mean`*0.001),
         date=as_date(dateTime)) %>% 
  select(-`system.time_start_mean`) %>% as_tibble() %>% 
  filter(Red_count/max.pix >=0.5) 
  #lastly, remove coastal islands by fid.
  
rm(lakes.filter, all.lakes, all.channels)
gc()

channels.combo = channels.filter%>% as_tibble() %>% 
  rename(Blue_chan_m = Blue_mean,Blue_chan_p10 = Blue_p10,Blue_chan_p90 = Blue_p90 ,
                 Gb_chan_m = Gb_ratio_mean, Gb_chan_p10 = Gb_ratio_p10,Gb_chan_p90 = Gb_ratio_p90,
                 Green_chan_m=Green_mean,Green_chan_p10=Green_p10,Green_chan_p90=Green_p90, 
                 Ndssi_chan_m=Ndssi_mean,Ndssi_chan_p10=Ndssi_p10, Ndssi_chan_p90=Ndssi_p90, 
                 Nsmi_chan_m=Nsmi_mean,Nsmi_chan_p10=Nsmi_p10, Nsmi_chan_p90=Nsmi_p90,
                 Red_chan_m=Red_mean, Red_chan_p10=Red_p10, Red_chan_p90=Red_p90, count_chan= Red_count) 
rm(channels.filter)
gc()

# Join lake and channel data from the same date and calculate the dominant wavelength ratio
lakes.combo=data.table(lakes.combo) %>% 
  group_by(OBJECTID, date) %>%  #only need to do if we have more than 1 obs per day (pick one with most pixels)
  mutate(my_ranks = order(Red_count, decreasing=TRUE)) %>% 
  filter(my_ranks==1) 
lakes.combo=data.table(lakes.combo)
channels.combo=data.table(channels.combo) %>% group_by(fid, date) %>%  #only need to do if we have more than 1 obs per day (pick one with most pixels)
  mutate(my_ranks = order(count_chan, decreasing=TRUE)) %>% 
  filter(my_ranks==1)
channels.combo=data.table(channels.combo)
gc()
# Figure out which lakes go with which island
lake.island.key = lakes.sf %>% st_transform(islands.sf %>% st_crs()) %>% st_join(islands.sf, left=T) %>% filter(!is.na(fid)) %>% select(fid, OBJECTID)



joined.df = lakes.combo %>% left_join(lake.island.key, by="OBJECTID") %>% select(-dateTime, -my_ranks, -geometry, -area_m) %>% 
  left_join(channels.combo, by=c("fid", "date")) %>% filter(!is.na(Red_chan_m))
m_lake_r = joined.df$Red_mean/1000
m_lake_g = joined.df$Green_mean/1000
m_lake_b = joined.df$Blue_mean/1000
dw_lake = chroma(m_lake_r, m_lake_g, m_lake_b)

m_chan_r = joined.df$Red_chan_m/1000
m_chan_g = joined.df$Green_chan_m/1000
m_chan_b = joined.df$Blue_chan_m/1000
dw_chan = chroma(m_chan_r, m_chan_g, m_chan_b)
joined.df$dom_wv_lake_m = dw_lake
joined.df$dom_wv_chan_m = dw_chan

# calculate ratios
channels.lakes = joined.df %>% 
  mutate(dom_wv_ratio_m = dom_wv_lake_m/dom_wv_chan_m,
         #dom_wv_ratio_p10 =dom_wv_lake_p10/dom_wv_chan_p10,
         #dom_wv_ratio_p90 =dom_wv_lake_p90/dom_wv_chan_p90,
         G_ratio_m = Green_mean/Green_chan_m, 
         B_ratio_m = Blue_mean/Blue_chan_m,
         R_ratio_m = Red_mean/Red_chan_m,
         Gb_ratio_m = Gb_ratio_mean/Gb_chan_m,
         Ndssi_ratio_m = Ndssi_mean/Ndssi_chan_m,
         Nsmi_ratio_m =Nsmi_mean/Nsmi_chan_m) %>% 
         #lci_m = (dom_wv_chan_m-dom_wv_lake_m)/(dom_wv_chan_m+dom_wv_lake_m)) %>% 
  filter(!is.na(Nsmi_ratio_m)) %>%  #Doesn't matter which joined variable you use here--just filtering those that don't have a match
  select(-dateTime, -delta, -my_ranks) %>% 
  mutate(year=year(date))
rm(channels.combo, lakes.combo, joined.df)
gc()

setwd("C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/intermediaryDownloads")
write_rds(channels.lakes, "joinedData_20220526.Rdata")

```

# Prep data for  testing and training
```{r}
setwd("C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/intermediaryDownloads")
channels.lakes=read_rds("joinedData_20220526.Rdata")

goodLakes.val = channels.lakes %>% as_tibble() %>% filter(year %in% highest.maxDis.yrs) %>% 
  group_by(OBJECTID) %>% summarise(no_obs = n()) %>% filter(no_obs>10)
goodLakes.val.ids = goodLakes.val$OBJECTID


# For each lake in  the validation period, calculate the median, sdev, and kurtosis for each possible band combination. Filter to just the data in the GECI validation period, and remove lakes who's connectivvity was classified as unclear in the GECI dataset. 
set.seed(1)
prep= channels.lakes %>% as_tibble() %>% filter(OBJECTID %in% goodLakes.val.ids) %>% 
  filter(year %in%  highest.maxDis.yrs) %>% 
  dplyr::group_by(OBJECTID) %>% 
  dplyr::summarise(med_dw_rat_m = median(dom_wv_ratio_m),
            sdev_dw_rat_m = sd(dom_wv_ratio_m),
            kurt_dw_rat_m = kurtosis(dom_wv_ratio_m),
           med_R_ratio_m = median(R_ratio_m),
           sdev_R_ratio_m = sd(R_ratio_m),
           kurt_R_ratio_m = kurtosis(R_ratio_m),
           med_B_ratio_m = median(R_ratio_m),
           sdev_B_ratio_m = sd(R_ratio_m),
           kurt_B_ratio_m = kurtosis(R_ratio_m),
           med_G_ratio_m = median(R_ratio_m),
           sdev_G_ratio_m = sd(R_ratio_m),
           kurt_G_ratio_m = kurtosis(R_ratio_m),
           med_Gb_ratio_m = median(Gb_ratio_m),
           sdev_Gb_ratio_m = sd(Gb_ratio_m),
           kurt_Gb_ratio_m = kurtosis(Gb_ratio_m),
           med_Ndssi_ratio_m = median(Ndssi_ratio_m),
           sdev_Ndssi_ratio_m = sd(Ndssi_ratio_m),
           kurt_Ndssi_ratio_m = kurtosis(Ndssi_ratio_m),
           med_Nsmi_ratio_m = median(Nsmi_ratio_m),
           sdev_Nsmi_ratio_m = sd(Nsmi_ratio_m),
           kurt_Nsmi_ratio_m = kurtosis(Nsmi_ratio_m)) %>% ungroup() 

mat=prep  %>% 
  select(med_dw_rat_m,sdev_dw_rat_m, med_Nsmi_ratio_m, sdev_Nsmi_ratio_m,
         med_Gb_ratio_m, sdev_Gb_ratio_m) %>% filter(sdev_Gb_ratio_m<0.4) %>% 
  rename("med dw ratio"="med_dw_rat_m",
         "sdev dw ratio"="sdev_dw_rat_m",
         "med NSMI ratio"="med_Nsmi_ratio_m",
         "sdev NSMI ratio"="sdev_Nsmi_ratio_m",
         "med G/B ratio"="med_Gb_ratio_m",
         "sdev G/B ratio"="sdev_Gb_ratio_m")
pairs(mat)

```

#Supervized classification 
```{r}
# Import GECI validation data for training/testing
setwd(valFile.path)
lake.class = st_read(valFileName , stringsAsFactors = F) %>% as_tibble() %>% 
  rename(OBJECTID=ID)
lake.class %>% group_by(type) %>% count()
# group classes by functional connectivity
lake.class.grouped=lake.class %>% mutate(final_class = case_when(
  type == "g1" | type== "g7" | type== "g4" ~ "1", #always low functional
  type== "g3_5" ~ "2", #some at high dis, none at low dis
  type == "g3"  | type =="g6" ~ "3", #high at high dis, none at low dis
  type == "g2_5" ~ "4", #high at high dis, some at low dis
  type == "g2" | type=="g5" ~ "5", # high at both low and high dis
  type== "badImage" | type == "coastal" | type =="notLake" | type=="uncertain" ~ "remove",
  type=="mediumThenHigh" | type=="moderateBoth" ~ "not enough data")) %>% select(-geometry, -type)
lake.class.grouped %>% group_by(final_class) %>% count()

# Now, use Boruta, see below, to figure out which variables are important
#https://www.datacamp.com/community/tutorials/feature-selection-R-boruta
set.seed(90)
trainData <- prep %>% left_join(lake.class.grouped, by="OBJECTID") %>% 
  dplyr::filter(!is.na(final_class) & (final_class != "remove") & (final_class !="not enough data")) %>% 
  mutate(classes=as.factor(final_class)) %>% dplyr::select(-final_class) %>%  
  dplyr::select(-OBJECTID)
## remove unnecessary columns using Boruta
attach(trainData)
colnames(trainData)
## apply the boruta
boruta.train <- Boruta(classes~., data = trainData,
                       mcAdj = TRUE, pValue = 0.01,doTrace = 2,ntree = 50)
# print the results of the boruta
print(boruta.train)
boruta.train$finalDecision
# plot the results of the boruta
plot(boruta.train, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 
## Get rid of tentative (yellow on the plot)
boruta.bank=TentativeRoughFix(boruta.train)
plot(boruta.bank, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 

#Select the variables to use during the classification
important.cols=getSelectedAttributes(boruta.bank, withTentative = F) 
bank_df <- attStats(boruta.bank)
print(bank_df) #visualize variable importance


###Create a decision tree! Tutorial here: https://www.gmudatamining.com/lesson-13-r-tutorial.html
set.seed(123)
geci.val =  prep %>% left_join(lake.class.grouped, by="OBJECTID") %>% 
  dplyr::filter(!is.na(final_class) & (final_class != "remove") & (final_class !="not enough data")) %>% 
  mutate(classes=as.factor(final_class)) %>% 
  dplyr::select(-final_class) %>%  
  dplyr::select(c("OBJECTID","classes", all_of(important.cols)))


#look at the data to make sure it is as expected
skimr::skim(geci.val)
# split data into test and train
set.seed(234)
geci.split = initial_split(geci.val, strata=classes)
geci.train = training(geci.split)
geci.test = testing(geci.split)

# Prepare data for later 5-fold cross validation
geci.folds5 = vfold_cv(geci.train, v=5, strata = classes)
# get the recipe setup for pre-processing
geci.recipe = recipe(classes~., data=geci.train) %>% 
   update_role(OBJECTID, new_role = "OBJECTID")  ###########################Maybe exclude the ID instead of updating the role, then adding it later
geci.recipe %>% prep() %>% bake(new_data=geci.train)#print the training data
# Set up the model and what parameters we want to tune
tree_model = decision_tree(cost_complexity=tune(),
                           tree_depth=tune(),
                           min_n=tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
# setup the workflow with both the model and the preprocessing recipe
tree_workflow=workflow() %>% 
  add_model(tree_model) %>% 
  add_recipe(geci.recipe)
# Hyperparameter tuning
set.seed(345)
##Build a grid of parameter options
tree_grid = grid_regular(cost_complexity(), 
                         tree_depth(), 
                         min_n(), levels=5)
#tune the grid
tree_tuning = tree_workflow %>% 
  tune_grid(resamples = geci.folds5, grid = tree_grid)
## Show the top 5 best models based on roc_auc metric
tree_tuning %>% collect_metrics() %>% 
  ggplot()+geom_point(aes(x=min_n, y=mean, color=as.factor(tree_depth), size=cost_complexity), alpha=0.3)+facet_wrap(~.metric)
##select the best model based on accuracy (it is the same model whether we pick using accuracy or roc_auc)
best_tree = tree_tuning %>% 
  select_best(metric="roc_auc")
best_tree

# Finalize workflow with the new tuned parameters
final_tree_workflow = tree_workflow %>% finalize_workflow(best_tree)
# Fit the model to the training dataset
set.seed(456)
tree_wf_fit = final_tree_workflow %>% fit(data=geci.train)
tree_fit = tree_wf_fit %>% extract_fit_parsnip()#show the  model
# expore the model
library(vip)
library(rpart.plot)
vip(tree_fit) #variable importance
tree.plot =rpart.plot::rpart.plot(tree_fit$fit, roundint=FALSE, type=5, extra=2)
tree.plot
# setwd(figures.filePath)
# ggsave(tree.plot, width=6.5, height=3.14, filename=tree.figure.name)
# apply the model to the test set
tree_last_fit = final_tree_workflow %>% last_fit(geci.split)
tree_last_fit %>% collect_metrics()
tree_predictions=tree_last_fit %>% collect_predictions()
conf_mat(tree_predictions, truth = classes, estimate = .pred_class)
confusionMatrix(tree_predictions$.pred_class, tree_predictions$classes)

```

## same thing but for random forrest
```{r}
# pre-process
geci.rec =recipe(classes ~., data=geci.train) %>% 
  update_role(OBJECTID, new_role = "OBJECTID") 
geci.pre=prep(geci.rec)
geci.juiced = juice(geci.pre)
geci.juiced %>% count(classes)
#Make model specifications & get ready to tune
tune.spec = rand_forest(
  mtry=tune(), #when you are making leaves of the tree, how many do you sample at each split--all predictors or just a few
  trees = 500,
  min_n=tune()# How long do you keep splitting. How many datapoints have to be in a node before you stop splitting
  ) %>% 
  set_mode("classification") %>% set_engine("ranger") #ranger is just one way of doing random forest
tune.wf=workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(tune.spec)
# Train hyperparameters with 5-fold cross validation
set.seed(345)
geci.fold  = vfold_cv(geci.train, v=5, strata=classes)
## tune parameters
#doParallel::registerDoParallel()
tune.res=tune_grid(
  tune.wf,
  resamples=geci.fold,
  grid=20
)
## take a look at parameters
tune.res %>% collect_metrics() #look at all the metrics
tune.res %>% select_best("roc_auc") #select best accuracy
tune.res %>% 
  collect_metrics() %>% 
  filter(.metric=="roc_auc") %>% 
  select(mean, min_n, mtry) %>% 
  pivot_longer(min_n:mtry,
               values_to="value",
               names_to="parameter") %>% 
  ggplot(aes(value, mean, color=parameter))+
  geom_point(show.legend="FALSE")+
  facet_wrap(~parameter, scales="free_x")
# Tune again using info from prior tuning
set.seed(456)
rf.grid= grid_regular(
  mtry(range=c(0,5)),
  min_n(range=c(10,30)),
  levels = 5
)
regular.res=tune_grid(
  tune.wf,
  resamples=geci.fold,
  grid=rf.grid
)
##roc_auc
regular.res %>% 
  collect_metrics() %>% 
  filter(.metric=="roc_auc") %>% 
  mutate(min_n=factor(min_n)) %>% 
  ggplot(aes(mtry, mean, color=min_n))+geom_point()+geom_line(alpha=0.5, size=1.5)
##accuracy
regular.res %>% 
  collect_metrics() %>% 
  filter(.metric=="accuracy") %>% 
  mutate(min_n=factor(min_n)) %>% 
  ggplot(aes(mtry, mean, color=min_n))+geom_point()+geom_line(alpha=0.5, size=1.5)
# select the best option
best.acc =select_best(regular.res, "roc_auc")
final.rf=finalize_model(
  tune.spec,
  best.acc
)
# Check out variable importance for the model as a whole
library(vip)
final.rf %>% set_engine("ranger", importance="permutation") %>% 
  fit(classes~.,
      data = juice(geci.pre) %>% select(-OBJECTID)) %>% 
  vip(geom="point")
# see how the model does on the testing data
final.wf = workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(final.rf)
final.res=final.wf %>% last_fit(geci.split)
final.res %>% collect_metrics()
final.res %>% collect_predictions() %>% 
  mutate(correct=case_when(classes==.pred_class~"Correct", TRUE ~"Incorrect")) %>% 
  bind_cols(geci.test) %>% 
  ggplot(aes(x=med_Nsmi_ratio_m,y=med_Gb_ratio_m, color=correct))+
  geom_point(size=3, alpha=0.4)+labs(color=NULL)+scale_color_manual(values=c("gray80", "darkred"))+theme_bw()
final.pred.rf= final.res %>% collect_predictions()
conf_mat(final.pred.rf, truth = classes, estimate = .pred_class)
confusionMatrix(final.pred.rf$.pred_class, final.pred.rf$classes)


## plot the training/testing dataset on a map
### This is the fitted model to use on the other datasets
fitted.wf.rf= pluck(final.res, 6)[[1]]
train.ids = geci.train$OBJECTID
test.ids = geci.test$OBJECTID
rf.class = cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>% select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>% 
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test")) %>% 
  gather(key="class",value="group", -OBJECTID, -split ) %>% 
  left_join(lakes.sf %>% 
              select(-Shape_Leng, -Shape_Area, -count), 
            by="OBJECTID") %>% 
  st_as_sf()

ggplot(rf.class)+geom_bar(aes(x=group, fill=class), 
                          stat="count", position="dodge") +
  facet_wrap(~split)

```





```{r}
#Classify using the other method...
prep %>% ggplot()+geom_density(aes(x=med_Nsmi_ratio_m))
classifications=prep %>% 
  mutate(connectivity=ifelse(med_Nsmi_ratio_m>=0.98 &  
                                 sdev_Gb_ratio_m<0.24, 
                             "high functional connectivity", "low functional connectivity"))


# do the "through time" classification
connectivity.groups = classifications%>% dplyr::select(ID, time_period, connectivity) %>% 
  spread(time_period, connectivity)


# get lakes that are always high functional connectivity
always.high.ids=connectivity.groups %>% 
  filter(`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")
always.high.ids=always.high.ids$ID

#get lakes that are always low functional connectivity
always.low.ids = connectivity.groups %>% 
  filter(`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
always.low.ids=always.low.ids$ID

# get lakes that go from high to low connectivity over time
high.to.low.ids = connectivity.groups %>% 
  filter((`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity") |
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
          (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
           )
high.to.low.ids=high.to.low.ids$ID

# get lakes that go from low to high connectivity over time
low.to.high.ids = connectivity.groups %>% 
  filter((`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity") |
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity"))
low.to.high.ids=low.to.high.ids$ID

# get lakes that flip back and forth connectivity through time
flip.ids = connectivity.groups %>% 
  filter((`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity") |
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
          (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
         (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
         )
flip.ids = flip.ids$ID

# classify connectivity variability through time
final.lakes=connectivity.groups %>% mutate(
  group = case_when(
    ID %in% always.high.ids ~ "always high functional connectivity",
    ID %in% always.low.ids ~ "always low functional connectivity",
    ID %in% high.to.low.ids ~ "high to low functional connectivity over time",
    ID %in% low.to.high.ids ~ "low to high functional connectivity over time",
    ID %in% flip.ids ~ "connectivity switches back and forth through time"
  )
) %>% filter(!is.na(group)) %>%
  mutate(group2 = ifelse(ID %in%flip.ids| 
                           ID %in% low.to.high.ids |
                           ID %in% high.to.low.ids, 
                         "variable functional connectivity", group))  

# import shapefile
setwd("C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/shapeFiles")
mack.lakes.sf = st_read("mackenzieGoodLakes.shp") %>% rename(ID=OBJECTID)
set.seed(100)
sample.mack.lakes = mack.lakes.sf %>% sample_n(length(mack.lakes.sf$ID)*0.2) %>% 
  mutate(fxd_ndx=row_number())
st_write(sample.mack.lakes, "mackenzieLakes20pct.shp")

#join shapefile to results
results.sf = final.lakes %>% left_join(mack.lakes.sf, by="ID") %>% st_as_sf()
mapview(results.sf, zcol="group")
```

