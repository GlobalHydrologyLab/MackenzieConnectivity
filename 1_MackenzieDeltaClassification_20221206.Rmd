---
title: "1_MackenzieDeltaConnectivityAnalysis_classGeneration"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Libraries
```{r}
library(tidyverse)
library(sf)
library(lubridate)
library(grDevices)
library(mapview)
library(extrafont)
library(ggpubr)
library(ggmap)
library(RgoogleMaps)
library(broom)
library(HistDAWass)
library(tidyhydat)
library(sp)
library(data.table)
library(ggalluvial)
library(gridExtra)
library(grid)
library(cowplot)
library(patchwork)
library(magick)
library(units)
library(Kendall)
library(ggspatial)
#Import libraries for Random Forest
library(caret) 
library(e1071)
library(Boruta)
library(tidymodels)
library(skimr)
library(vip)
```

# Functions
```{r}
#takes RGB to calculate dominant wavelength
chroma <- function(R, G, B) {  
  require(colorscience)

# Convert R,G, and B spectral reflectance to dominant wavelength #based
# on CIE chromaticity color space

# see Wang et al 2015. MODIS-Based Radiometric Color Extraction and
# Classification of Inland Water With the Forel-Ule
# Scale: A Case Study of Lake Taihu

# chromaticity.diagram.color.fill()
Xi <- 2.7689*R + 1.7517*G + 1.1302*B
Yi <- 1.0000*R + 4.5907*G + 0.0601*B
Zi <- 0.0565*G + 5.5943*B

# calculate coordinates on chromaticity diagram
x <-  Xi / (Xi + Yi +  Zi)
y <-  Yi / (Xi + Yi +  Zi)
z <-  Zi / (Xi + Yi +  Zi)

# calculate hue angle
alpha <- atan2( (x - (1/3)), (y - (1/3))) * 180/pi

# make look up table for hue angle to wavelength conversion
cie <- cccie31 %>%
  dplyr::mutate(a = atan2( (x - (1/3)), (y - (1/3))) * 180/pi) %>%
  dplyr::filter(wlnm <= 700) %>%
  dplyr::filter(wlnm >=380) 

# find nearest dominant wavelength to hue angle
wl <- cie[as.vector(sapply(alpha,function(x) which.min(abs(x - cie$a)))) , 'wlnm']

return(wl)
}
```

# Imports and file paths
```{r}
# dates for version control
stage1.date = "20230104" # the first data join phase
stage2.date = "20230104" # The classified training testing data via random forest
stage3.date = "20230104" # Classification of all mackenzie delta lakes
stage4.date = "20230104" # date of export of gif

# Names of files and folders for reflectance data
import.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/GEE Downloads"
import.Lakes = "MackenzieLakeExport_20230104.csv"
import.channels = "MackenzieChannelExport_20230105.csv"
import.sword = "na_sword_reaches_hb82_v14.shp"

# intermediate working directory
int.wd="C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/intermediaryDownloads"


#Name of file and folder for lake shapefiles & island polygon shapefiles
shapeFiles.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/shapeFiles"
lakes.shapeFile = "mackenzieGoodLakes.shp"
islands.shapeFile = "vectorIslandArea2.shp"
setwd(shapeFiles.filePath)
lakes.sf = st_read(lakes.shapeFile)
islands.sf=st_read(islands.shapeFile)
gif.wd = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/images/GIF_20230105"
images.wd = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/images"
pil.wd = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/PiliourasAndRowland/"
# 
# # Name of file and folder for GECI validation data
valFile.path="C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/GEE Downloads/trainingData"
valFileName = "trainingData_1819lakes.shp"
# 
# # Name of file and folder for figures
# figures.filePath = "C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Papers/ColvilleDeltaConnectivity/Figures/Figures_raw"
# dens.figure.name = "densityPlotExample.pdf"
# tree.figure.name = "DecisionTreeFigure.pdf"

```


# Import and filter the lake and channel data  - calculates both for mean, tenth percentile and 90% dom wv values.
```{r}
setwd(import.filePath)
#import lake data
all.lakes = read.csv(import.Lakes) %>% dplyr::as_tibble()
#import all lake buffer/channel file
all.channels = read.csv(import.channels) %>% dplyr::as_tibble()


# Filter lake and channel data
lakes.maxPix = all.lakes %>% group_by(OBJECTID) %>% summarise(max.pix = max(Red_count))# count max number of pix--> likely the clear sky total of the lake
lakes.filter = all.lakes %>% 
  left_join(lakes.maxPix, by="OBJECTID") %>% 
  mutate(dateTime = as_datetime(system.time_start_mean*0.001),
         date = as_date(dateTime)) %>% 
  dplyr::select(-system.time_start_mean, -count) %>% 
  dplyr::filter(Red_count/max.pix >= 0.5)  # each observation must be made up of at least 50% of the lake area
lakes.combo=lakes.filter
channels.maxPix = all.channels %>% group_by(fid) %>% summarise(max.pix = max(Red_count))
channels.filter = all.channels %>% 
  left_join(channels.maxPix, by="fid") %>% 
  mutate(dateTime = as_datetime(`system.time_start_mean`*0.001),
         date=as_date(dateTime)) %>% 
  dplyr::select(-`system.time_start_mean`) %>% as_tibble() %>% 
  dplyr::filter(Red_count/max.pix >=0.5) 
  #lastly, remove coastal islands by fid.
  
rm(lakes.filter, all.lakes, all.channels)
gc()

channels.combo = channels.filter%>% as_tibble() %>% 
  rename(Blue_chan_m = Blue_mean,Blue_chan_p10 = Blue_p10,Blue_chan_p90 = Blue_p90 ,
                 Gb_chan_m = Gb_ratio_mean, Gb_chan_p10 = Gb_ratio_p10,Gb_chan_p90 = Gb_ratio_p90,
                 Green_chan_m=Green_mean,Green_chan_p10=Green_p10,Green_chan_p90=Green_p90, 
                 Ndssi_chan_m=Ndssi_mean,Ndssi_chan_p10=Ndssi_p10, Ndssi_chan_p90=Ndssi_p90, 
                 Nsmi_chan_m=Nsmi_mean,Nsmi_chan_p10=Nsmi_p10, Nsmi_chan_p90=Nsmi_p90,
                 Nir_chan_m = Nir_mean, Nir_chan_p10 = Nir_p10, Nir_chan_p90=Nir_p90,
                 Ndti_chan_m = Ndti_mean, Ndti_chan_p10 = Ndti_p10, Ndti_chan_p90=Ndti_p90,
                 Nsmi_mod_chan_m = Nsmi_mod_mean, Nsmi_mod_chan_p10=Nsmi_mod_p10, Nsmi_mod_chan_p90=Nsmi_mod_p90,
                 Red_chan_m=Red_mean, Red_chan_p10=Red_p10, Red_chan_p90=Red_p90, count_chan= Red_count) 
rm(channels.filter)
gc()

# Join lake and channel data from the same date and calculate the dominant wavelength ratio
lakes.combo=data.table(lakes.combo) %>% 
  group_by(OBJECTID, date) %>%  #only need to do if we have more than 1 obs per day (pick one with most pixels)
  mutate(my_ranks = order(Red_count, decreasing=TRUE)) %>% 
  filter(my_ranks==1) 
lakes.combo=data.table(lakes.combo)
channels.combo=data.table(channels.combo) %>% group_by(fid, date) %>%  #only need to do if we have more than 1 obs per day (pick one with most pixels)
  mutate(my_ranks = order(count_chan, decreasing=TRUE)) %>% 
  filter(my_ranks==1)
channels.combo=data.table(channels.combo)
gc()
# Figure out which lakes go with which island
lake.island.key = lakes.sf %>% st_transform(islands.sf %>% st_crs()) %>% st_join(islands.sf, left=T) %>% dplyr::filter(!is.na(fid)) %>% dplyr::select(fid, OBJECTID)


# Perfom the join
joined.df = lakes.combo %>% left_join(lake.island.key, by="OBJECTID") %>% dplyr::select(-dateTime, -my_ranks, -geometry, -area_m) %>% 
  left_join(channels.combo, by=c("fid", "date")) %>% filter(!is.na(Red_chan_m)) #filter out data with NA reflectance values

# Calculate dominate wavelength ratio values for lakes and channels
## lakes
m_lake_r = joined.df$Red_mean/1000
m_lake_g = joined.df$Green_mean/1000
m_lake_b = joined.df$Blue_mean/1000
dw_lake = chroma(m_lake_r, m_lake_g, m_lake_b)
## Channels
m_chan_r = joined.df$Red_chan_m/1000
m_chan_g = joined.df$Green_chan_m/1000
m_chan_b = joined.df$Blue_chan_m/1000
dw_chan = chroma(m_chan_r, m_chan_g, m_chan_b)
joined.df$dom_wv_lake_m = dw_lake
joined.df$dom_wv_chan_m = dw_chan

# calculate ratios
channels.lakes = joined.df %>% 
  mutate(dom_wv_ratio_m = dom_wv_lake_m/dom_wv_chan_m,
         G_ratio_m = Green_mean/Green_chan_m, 
         B_ratio_m = Blue_mean/Blue_chan_m,
         R_ratio_m = Red_mean/Red_chan_m,
         Nir_ratio_m = Nir_mean/Nir_chan_m,
         Gb_ratio_m = Gb_ratio_mean/Gb_chan_m,
         Ndssi_ratio_m = Ndssi_mean/Ndssi_chan_m,
         Nsmi_ratio_m =Nsmi_mean/Nsmi_chan_m,
         Nsmi_mod_ratio_m = Nsmi_mod_mean/Nsmi_mod_chan_m,
         Ndti_ratio_m = Ndti_mean/Ndti_chan_m) %>% 
  dplyr::filter(!is.na(Nsmi_ratio_m)) %>%  #Doesn't matter which joined variable you use here--just filtering those that don't have a match
  dplyr::select(-dateTime, -delta, -my_ranks) %>% 
  mutate(year=year(date))
gc()

setwd(int.wd)
write_rds(channels.lakes, paste0("joinedData_", stage1.date,".Rdata"))
write_rds(channels.combo, paste0("channelData_", stage1.date,".Rdata"))
write_rds(lakes.combo, paste0("lakeData_", stage1.date,".Rdata"))



```


# Prep data for  testing and training
```{r}
# Import the joined lake/channel data
setwd(int.wd)
channels.lakes=read_rds(paste0("joinedData_", stage1.date,".Rdata")) 

# For each lake in  the validation period, calculate the median, sdev, and kurtosis for each possible band combination. Filter to just the data in the GECI validation period (2020), and remove lakes who's connectivity was classified as unclear in the GECI dataset. 
set.seed(1)
prep.2020= channels.lakes%>%
  mutate(group = case_when(
    (yday(date)>=162 &yday(date)<=182) ~ "high discharge", #training imagery from high discharge period in 2020
    (yday(date)>235 & yday(date)<=254)~"low discharge" # training imagery from low discharge period in 2020
  )) %>% drop_na() %>% filter(year==2020) %>% 
  as_tibble() %>%
  dplyr::group_by(OBJECTID, year, group) %>%
  dplyr::summarise(med_dw_rat_m = median(dom_wv_ratio_m),
          sdev_dw_rat_m = sd(dom_wv_ratio_m),
          kurt_dw_rat_m = kurtosis(dom_wv_ratio_m),
           med_R_ratio_m = median(R_ratio_m),
           sdev_R_ratio_m = sd(R_ratio_m),
           kurt_R_ratio_m = kurtosis(R_ratio_m),
           med_B_ratio_m = median(B_ratio_m),
           sdev_B_ratio_m = sd(B_ratio_m),
           kurt_B_ratio_m = kurtosis(B_ratio_m),
           med_G_ratio_m = median(G_ratio_m),
           sdev_G_ratio_m = sd(G_ratio_m),
           kurt_G_ratio_m = kurtosis(G_ratio_m),
           med_Nir_ratio_m = median(Nir_ratio_m),
           sdev_Nir_ratio_m = sd(Nir_ratio_m),
           kurt_Nir_ratio_m = kurtosis(Nir_ratio_m),
           med_Gb_ratio_m = median(Gb_ratio_m),
           sdev_Gb_ratio_m = sd(Gb_ratio_m),
           kurt_Gb_ratio_m = kurtosis(Gb_ratio_m),
           med_Ndssi_ratio_m = median(Ndssi_ratio_m),
           sdev_Ndssi_ratio_m = sd(Ndssi_ratio_m),
           kurt_Ndssi_ratio_m = kurtosis(Ndssi_ratio_m),
           med_Nsmi_ratio_m = median(Nsmi_ratio_m),
           sdev_Nsmi_ratio_m = sd(Nsmi_ratio_m),
           kurt_Nsmi_ratio_m = kurtosis(Nsmi_ratio_m),
           med_Nsmi_mod_ratio_m = median(Nsmi_mod_ratio_m),
           sdev_Nsmi_mod_ratio_m = sd(Nsmi_mod_ratio_m),
           kurt_Nsmi_mod_ratio_m = kurtosis(Nsmi_mod_ratio_m),
           med_Ndti_ratio_m = median(Ndti_ratio_m),
           sdev_Ndti_ratio_m = sd(Ndti_ratio_m),
           kurt_Ndti_ratio_m = kurtosis(Ndti_ratio_m),
           count=n(),
           ) %>% ungroup() %>% dplyr::filter(count>=2) # require at least 2 or more observations in each period

```

# Supervized classification - Random Forest & Training/Testing evaluation
```{r}
# Import training/testing data into the script, break our much larger classification scheme into 3 classes
setwd(valFile.path)
lake.class = st_read(valFileName , stringsAsFactors = F) %>% as_tibble() %>% 
  rename(OBJECTID=ID) %>% 
  group_by(OBJECTID) %>% mutate(row.num=row_number()) %>% filter(row.num==1) %>% 
  dplyr::select(-row.num) %>% 
  ungroup() %>% 
  mutate(high.dis.class = case_when(
   type == "g1" | type== "g7" | type== "g4" | type=="lowThenMedium" ~ 0,
   type== "g3_5" | type=="mediumThenHigh" | type == "moderateBoth"~ 1,
   type == "g2_5" | type == "g3"  | type =="g6" | type == "g2" | type=="g5" |type=="g2_add" ~ 2
  ),
  low.dis.class = case_when(
    type == "g1" | type== "g7" | type== "g4" | type== "g3_5" | type == "g3"  | type =="g6"~ 0 ,
    type == "g2_5" | type == "moderateBoth" | type=="lowThenMedium" ~ 1,
    type == "g2" | type=="g5" |type=="g2_add" | type=="mediumThenHigh" ~ 2
  )
  ) %>% 
  gather(Key, classes, -OBJECTID, -type, -geometry) %>% 
  mutate(group = ifelse(Key=="high.dis.class", "high discharge", "low discharge")) %>% 
  dplyr::select(-Key, -type, -geometry) %>% 
  dplyr::filter(!is.na(classes) )  # Remove NA class values -- includes lakes whose class was obscured by clouds or bad imagery

# Join the remote sensing data to the lake classificatin data
set.seed(90)
trainData <- prep.2020 %>%   
  left_join(lake.class, by=c("OBJECTID", "group")) %>% 
  dplyr::select(-OBJECTID, -year, -count, -group) %>% drop_na() 


# Investigate reflectance variables that are correlated to each other, remove high correlations
## Plot all correlations
corrplot::corrplot(trainData  %>% as.matrix() %>% cor(), order="hclust", method="shade", diag=F)
## Remove correlations
prep.clust = trainData %>% dplyr::select(-classes) # remove all non reflectance variables
prep.clust
# right now we do the 'order' thing because it picks which vars are correlated and picks whichever of correlated variables come first to use and discards the other. I wanted it to at least do med_R_ratio rather than med_G_ratio, so I ordered it this way. Could be better coded.
prep.clust=prep.clust[, order(names(prep.clust), decreasing=T)]
clust = findCorrelation(prep.clust %>%  cor(method="spearman"), 
                        cutoff=0.95, verbose=T, names=F)
clust 
clust = sort(clust)
# Actually remove the duplicate variables
geci.clust = trainData[, -c(clust)]
geci.clust

# Boruta analysis to identify important variables 
## remove unnecessary columns using Boruta
attach(geci.clust)
colnames(geci.clust)
## apply the boruta
boruta.train <- Boruta(classes~., data = geci.clust,
                       mcAdj = TRUE, pValue = 0.01,doTrace = 2,ntree = 50)
## print the results of the boruta
print(boruta.train)
boruta.train$finalDecision
## plot the results of the boruta
plot(boruta.train, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 
## Get rid of tentative (yellow on the plot)
boruta.bank=TentativeRoughFix(boruta.train)
plot(boruta.bank, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 

##Select the variables to use during the classification
important.cols=getSelectedAttributes(boruta.bank, withTentative = F) 
bank_df <- attStats(boruta.bank)
print(bank_df) #visualize variable importance  
bank_df$variable = rownames(bank_df)
bank_df=bank_df %>% arrange(medianImp)
bank_df$variable = factor(bank_df$variable, levels=bank_df$variable)

ggplot(data=bank_df %>% as_tibble()) + 
  geom_point(aes(x=variable, y=medianImp, color=decision), size=3)+theme_bw()+
  theme(axis.text.x=element_text(hjust=1, angle=45, size=12))+
  ylab("median importance")+xlab("")+
  theme(axis.title = element_text(size=14, face="bold"),
        legend.title = element_text(size=14, face="bold"),
        legend.text = element_text(size=12))+labs(color="Decision")

# Prep data for random forest classification, 
set.seed(123)
## Join the reflectance and lake classification data,keep only important less correlated columns
geci.val =  prep.2020 %>% 
  left_join(lake.class, by=c("OBJECTID", "group")) %>% 
  dplyr::select(c("OBJECTID","classes", all_of(important.cols))) %>% drop_na() %>% 
  mutate(classes=as.factor(classes))
## look at the data to make sure it is as expected
skimr::skim(geci.val)
## split data into test and train
set.seed(234)
geci.split = initial_split(geci.val, strata=classes)
geci.train = training(geci.split)
geci.test = testing(geci.split)

# Start random forest classification
geci.rec =recipe(classes ~., data=geci.train) %>% 
  update_role(OBJECTID, new_role = "OBJECTID") 
geci.pre=prep(geci.rec)
geci.juiced = juice(geci.pre)
geci.juiced %>% count(classes)
## Make model specifications & get ready to tune
tune.spec = rand_forest(
  mtry=tune(), #when you are making leaves of the tree, how many do you sample at each split--all predictors or just a few
  trees = 500,
  min_n=tune()# How long do you keep splitting. How many data points have to be in a node before you stop splitting
  ) %>% 
  set_mode("classification") %>% set_engine("ranger") #ranger is just one way of doing random forest
tune.wf=workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(tune.spec)
## Train hyperparameters with 5-fold cross validation
set.seed(345)
geci.fold  = vfold_cv(geci.train, v=5, strata=classes)
## tune parameters
tune.res=tune_grid(
  tune.wf,
  resamples=geci.fold,
  grid=20
)
## take a look at parameters
tune.res %>% collect_metrics() #look at all the metrics
tune.res %>% select_best("roc_auc") #select best accuracy
tune.res %>% 
  collect_metrics() %>% 
  dplyr::filter(.metric=="roc_auc") %>% 
  dplyr::select(mean, min_n, mtry) %>% 
  pivot_longer(min_n:mtry,
               values_to="value",
               names_to="parameter") %>% 
  ggplot(aes(value, mean, color=parameter))+
  geom_point(show.legend="FALSE")+
  facet_wrap(~parameter, scales="free_x")+theme_bw()+
  theme(axis.text = element_text(size=12),
        axis.title = element_text(size=14, face="bold"),
        strip.text = element_text(size=14, face="bold"))+ylab("mean ROC AUC ")

set.seed(456)

## select the best option based on the ROC_AUC parameter
best.acc =select_best(tune.res, "roc_auc") 
final.rf=finalize_model(
  tune.spec,
  best.acc
)
## Check out variable importance for the model as a whole
final.rf %>% set_engine("ranger", importance="permutation") %>% 
  fit(classes~.,
      data = juice(geci.pre) %>% dplyr::select(-OBJECTID)) %>% 
  vip(geom="point")+theme_bw()
# see how the model does on the testing data
final.wf = workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(final.rf)
final.res=final.wf %>% last_fit(geci.split)
final.res %>% collect_metrics()
final.res %>% collect_predictions() 


# Select the final model & apply it to the training/testing dataset
fitted.wf.rf= pluck(final.res, 6)[[1]]
train.ids = geci.train$OBJECTID
test.ids = geci.test$OBJECTID
final.pred.cm = cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>% 
  dplyr::select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>% 
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test"))

## Save the training/test classifications
setwd(int.wd)
write_rds(final.pred.cm, paste0("predictions_traintest_",stage2.date, ".Rdata"))

## Plot a confusion matrix
### Just testing split
table.test=confusionMatrix(final.pred.cm[final.pred.cm$split=="test",]$.pred_class,
                      final.pred.cm[final.pred.cm$split=="test",]$.obs_class)
### All training and testing
table.all=confusionMatrix(final.pred.cm$.pred_class,
                      final.pred.cm$.obs_class)

table.test
table.all


# Plot ggalluvial plot showing classification for the testing data (actual vs predicted)
## Prep the data
alluvial.prep= cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>%  # apply the model
  dplyr::select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>%  # shape the data
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test")) %>% # group the data into training vs testing
  dplyr::filter(split=="test") %>%  # select only test data
  group_by(.pred_class, .obs_class) %>% summarise(freq=n()) 
## Create the plot
ggplot(data=alluvial.prep, aes(axis1=.obs_class, axis2 = .pred_class, y=freq))+
  geom_alluvium(aes(fill=.pred_class))+geom_stratum(aes(fill=.pred_class))+
  scale_fill_manual(values=c("#619CFF", "#00BA38", "#F8766D"))+
  geom_text(stat = "stratum",
            aes(label = after_stat(stratum))) +
  scale_x_discrete(limits = c("Survey", "Response"),
                   expand = c(0.15, 0.05)) +
  theme_void()+theme(legend.title=element_blank())

```


# Apply random forest model to all lakes in all time periods
```{r}
# import the data and get it prepped for the classification
setwd(int.wd)
all.data = read_rds(paste0("joinedData_", stage1.date,".Rdata")) %>% 
  mutate(year=year(date), month=month(date))
prep.all= all.data %>% as_tibble() %>% 
  dplyr::group_by(OBJECTID, year, month) %>% # group by object, id, and month
  dplyr::summarise(med_dw_rat_m = median(dom_wv_ratio_m),
          sdev_dw_rat_m = sd(dom_wv_ratio_m),
          kurt_dw_rat_m = kurtosis(dom_wv_ratio_m),
           med_R_ratio_m = median(R_ratio_m),
           sdev_R_ratio_m = sd(R_ratio_m),
           kurt_R_ratio_m = kurtosis(R_ratio_m),
           med_B_ratio_m = median(B_ratio_m),
           sdev_B_ratio_m = sd(B_ratio_m),
           kurt_B_ratio_m = kurtosis(B_ratio_m),
           med_G_ratio_m = median(G_ratio_m),
           sdev_G_ratio_m = sd(G_ratio_m),
           kurt_G_ratio_m = kurtosis(G_ratio_m),
           med_Nir_ratio_m = median(Nir_ratio_m),
           sdev_Nir_ratio_m = sd(Nir_ratio_m),
           kurt_Nir_ratio_m = kurtosis(Nir_ratio_m),
           med_Gb_ratio_m = median(Gb_ratio_m),
           sdev_Gb_ratio_m = sd(Gb_ratio_m),
           kurt_Gb_ratio_m = kurtosis(Gb_ratio_m),
           med_Ndssi_ratio_m = median(Ndssi_ratio_m),
           sdev_Ndssi_ratio_m = sd(Ndssi_ratio_m),
           kurt_Ndssi_ratio_m = kurtosis(Ndssi_ratio_m),
           med_Nsmi_ratio_m = median(Nsmi_ratio_m),
           sdev_Nsmi_ratio_m = sd(Nsmi_ratio_m),
           kurt_Nsmi_ratio_m = kurtosis(Nsmi_ratio_m),
           med_Nsmi_mod_ratio_m = median(Nsmi_mod_ratio_m),
           sdev_Nsmi_mod_ratio_m = sd(Nsmi_mod_ratio_m),
           kurt_Nsmi_mod_ratio_m = kurtosis(Nsmi_mod_ratio_m),
           med_Ndti_ratio_m = median(Ndti_ratio_m),
           sdev_Ndti_ratio_m = sd(Ndti_ratio_m),
           kurt_Ndti_ratio_m = kurtosis(Ndti_ratio_m),
           count=n(),
           )  %>% ungroup() %>% dplyr::filter(count>=2) %>% # keep only groups with at least two dates going into the summerized value
  dplyr::select(c("OBJECTID","year","month" ,all_of(important.cols))) %>% drop_na()

# Apply the classification and convert resulting dataframe in to a spatial object
set.seed(500)
all.classified = cbind(predict(fitted.wf.rf, prep.all), prep.all) %>% as_tibble() %>% 
  dplyr::select(.pred_class, OBJECTID, year, month) %>% 
  left_join(lakes.sf %>% dplyr::select(-count), by="OBJECTID") %>% arrange(year, month)


setwd(int.wd)
write_rds(all.classified,paste0("final.class_", stage3.date, ".Rdata"))

```

# Plot connectivity results
```{r}
# Plot results
## set color palette
cols= c("2"="#F8766D", "1"="#00BA38" , "0"="#619CFF")
crs.plot = "+proj=tcea +lon_0=-134.3847656 +datum=WGS84 +units=m +no_defs"

# import river centerlines 
setwd(shapeFiles.filePath)
## study area and mack.basin are used to select river centerlines in the delta itself
study.area = st_bbox(all.classified %>% st_as_sf(), crs=4326) 
study.area = st_as_sfc(study.area) %>% 
  st_transform(crs = crs.plot)
mack.basin = st_read(import.sword) %>% 
  st_transform(crs = crs.plot) %>% 
  st_intersection(study.area) %>% filter(width>90)
## study.area.large and mack.basin.large include centerlines in a slightly larger area that encapsulates the WSC gage Mack River @ Arctic Red River
study.area.large=cbind.data.frame(lon=c(-136.80, -136.80, -133.47, -133.47), 
                 lat=c(67.46, 69.55, 69.55, 67.46)) %>% 
  st_as_sf(coords=c("lon", "lat")) %>% st_set_crs(4326) %>% st_bbox() %>% st_as_sfc() %>% 
  st_transform(crs = crs.plot)
mack.basin.large = st_read(import.sword) %>% 
  st_transform(crs = crs.plot) %>% 
  st_intersection(study.area.large) %>% filter(width>90)

# Plot median june and july results
setwd(int.wd)
all.classified=read_rds(paste0("final.class_", stage3.date, ".Rdata"))
all.classified %>% 
  mutate(.pred_class=as.numeric(as.character(.pred_class)))  %>% group_by(OBJECTID, month) %>% 
  summarise(mean.score=mean(.pred_class, na.rm=T),
            med.score = median(.pred_class, na.rm=T),
            count= n() ) %>% dplyr::filter(count>=10) %>% ungroup() %>% 
  left_join(lakes.sf %>% dplyr::select(-count), by="OBJECTID") %>% st_as_sf() %>% 
  st_transform(crs = crs.plot) %>% 
  dplyr::filter(month %in% c(6, 7)) %>% 
  mutate(month = case_when(
    month==6 ~factor("June", levels=c("June", "July")),
    month==7 ~ factor("July", levels=c("June", "July"))
  )) %>% 
  ggplot()+
  geom_sf(aes(fill=med.score), color=NA)+facet_wrap(~month)+
  scale_fill_gradient2(high = "#F8766D", mid="#00BA38",low = "#619CFF" , midpoint=1)+theme_bw()+
  annotation_scale(text_cex = 0.9)+
  geom_sf(data=mack.basin, color="grey65")+
  theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
        strip.text = element_text(size=14, face="bold"),
        axis.text.y = element_text(size=12),
        legend.title = element_text(size=14, face="bold"),
        legend.text=element_text(size=12), 
        legend.position="bottom")+labs(fill="Median Class")+
  guides(fill=guide_colourbar(direction="horizontal",  barwidth = 30, title.position = "top", title.hjust = 0.5))

# Create a gif with a map of june connectivity, discharge peak value and peak date at two locations
## Import discharge data
### Mack river at shallow bay
#### import mack river at shallow bay discharge
sb.dis = hy_daily_levels(station_number = "10MC023") %>% mutate(year=year(Date), month=month(Date),
                                                       doy = yday(Date)) %>% 
  filter(year>=2000) %>% filter(month>=3 & month<=7) %>% group_by(year) %>% nest() %>% filter(year!=2013 & year !=2008) # Remove years with a lot if missing data
#### calculate peak yearly discharge and date of yearly peak discharge at Mack River at Shallow Bay
max.doy.df.sb = NULL
for (i in 1:length(sb.dis$year)){
  dat = sb.dis$data[[i]] %>% filter(month<=7)
  yr = sb.dis$year[[i]]

  i.minmax = which.max(dat$Value)
  max.first.doy = dat$doy[i.minmax]
  max.first.value = dat$Value[i.minmax]
  
  max.doy.sub = cbind.data.frame(yr, max.first.doy, max.first.value)
  max.doy.df.sb = rbind.data.frame(max.doy.df.sb, max.doy.sub)
}
sb.dis.unnest = sb.dis %>% unnest(data)
#### Plot the yearly discharge and peak discharge calculations for mack river at Shallow Bay
ggplot()+geom_line(data= sb.dis.unnest, aes(x=doy, y=Value, group=year))+
  geom_vline(data=max.doy.df.sb %>% rename(year=yr), mapping=aes(xintercept= max.first.doy))+
  facet_wrap(~year)


### Mack river at Arctic red River
#### Import discharge at Mack River at Arctic Red River
rr.dis = hy_daily_levels(station_number = "10LC014") %>% mutate(year=year(Date), month=month(Date),
                                                       doy = yday(Date)) %>% 
  filter(year>=2000) %>% filter(month>=3 & month<=7) %>% group_by(year) %>% nest() %>% filter(year!=2019) # remove year with missing data
#### calculate peak yearly discharge and date of yearly peak discharge at Mack River at Red river
max.doy.df.rr = NULL
for (i in 1:length(rr.dis$year)){
  dat = rr.dis$data[[i]] %>% filter(month<=7)
  yr = rr.dis$year[[i]]

  i.minmax = which.max(dat$Value)
  max.first.doy = dat$doy[i.minmax]
  max.first.value = dat$Value[i.minmax]
  
  max.doy.sub = cbind.data.frame(yr, max.first.doy, max.first.value)
  max.doy.df.rr = rbind.data.frame(max.doy.df.rr, max.doy.sub)
}
rr.dis.unnest = rr.dis %>% unnest(data)
#### Plot the yearly discharge and peak discharge calculations for mack river at Arctic Red River
ggplot()+geom_line(data= rr.dis.unnest, aes(x=doy, y=Value, group=year))+
  geom_vline(data=max.doy.df.rr %>% rename(year=yr), mapping=aes(xintercept= max.first.doy))+
  facet_wrap(~year)+theme_bw()



## combine Red River and shallow Bay peak discharge observations into one data frame
combo.df = rbind(
  #max.doy.df.iv %>% mutate(rivr = "MACKENZIE RIVER (EAST CHANNEL) AT INUVIK"),
      max.doy.df.sb %>%  mutate(rivr="Mackenzie River (Napoiak Channel) Above Shallow Bay"),
      max.doy.df.rr %>% mutate(rivr = "Mackenzie River at Arctic Red River")) %>% 
  as_tibble() %>% mutate(year=yr)

## Retrieve the lat/lon of each station for easier plotting
sb.location = hy_stations(station="10MC023") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "Mackenzie River (Napoiak Channel) Above Shallow Bay") %>% 
  st_transform(crs = crs.plot)
rr.location = hy_stations(station="10LC014") %>% 
  st_as_sf(coords=c("LONGITUDE","LATITUDE")) %>% st_set_crs(4326) %>% 
  mutate(rivr = "Mackenzie River at Arctic Red River") %>% 
  st_transform(crs = crs.plot)

combo.location = rbind.data.frame(sb.location, rr.location) %>% st_as_sf()

## Create the gif
setwd(gif.wd)
prep.gif.data = all.classified %>% filter(month==6) %>% 
  st_as_sf() %>% st_transform(crs = crs.plot) %>% 
  group_by(year) %>% nest()
### Loop through each year
for (z in 1: length(prep.gif.data$year)){
  dat = prep.gif.data$data[[z]]
  year.main = prep.gif.data$year[[z]]
  
  #### Create plot 1 (map of June connectivity)
  p1 = ggplot(data=dat)+
   geom_sf(aes(fill=.pred_class), color=NA)+
    scale_fill_manual(values=c("#619CFF","#00BA38","#F8766D"))+theme_bw()+
    annotation_scale(text_cex = 0.9)+
    geom_sf(data=mack.basin.large, color="grey65")+
    geom_sf(data=study.area, color=NA, fill=NA)+
    geom_sf(data=combo.location, aes(color=rivr), size=5)+
    scale_colour_manual(guide="none",     values=c("#000000", "#ABA9A9"))+
    theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
          strip.text = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
         # legend.title = element_text(size=16, face="bold"),
          legend.text=element_text(size=16, face="bold"),
          title=element_text(size=18, face="bold"),
          legend.position="bottom", legend.direction="horizontal",
          legend.key.size=unit(1, "cm"),
          legend.title=element_blank(),
         legend.box.spacing = unit(0, "pt"),
         legend.margin=margin(0,0,0,0))+labs(fill="Class")+
    guides(fill=guide_legend(label.position="top",label.vjust = -8, title.vjust = 0.2))
  
  #### create plot 2 (date of peak water level)
  p2= combo.df %>% as_tibble() %>% ggplot()+geom_line(aes(x=yr, y=max.first.doy, color=rivr))+
    geom_point(aes(x=yr, y=max.first.doy,color=rivr), size=1.5)+theme_bw()+
    geom_point(data=combo.df %>% filter(year==year.main), aes(x=year, y=max.first.doy, color=rivr), size=5)+
    ylab(str_wrap("date of peak water level", width=10))+
    xlab("")+
    #geom_smooth(method=lm, aes(x=yr, y=max.first.value), se=F)+
    theme(axis.text.x = element_text( size=12),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(angle=45, hjust=1, vjust=-0.05,size=12),
          legend.title = element_blank(),
          legend.text=element_text(size=14),
          legend.position = "bottom")+
    scale_colour_manual(labels = function(x) str_wrap(x, width = 30),
                         values=c("#000000", "#ABA9A9") )+
    scale_y_continuous(breaks = c(121,127,135, 144,152, 160), labels = c("May 1st", "May 7th","May 15th",
                                                                         "May 24th", 
                                                                     "June 1st", "June 9th"))+
    labs(colour="WSC Station Name")+xlim(2000,2020)
  #### Create plot 3 (height of peak water level)
  p3=combo.df %>% as_tibble() %>% ggplot()+geom_line(aes(x=yr, y=max.first.value, color=rivr))+
    geom_point(aes(x=yr, y=max.first.value,color=rivr), size=1.5)+theme_bw()+
    geom_point(data=combo.df %>% filter(year==year.main), aes(x=year, y=max.first.value, color=rivr), size=5)+
    xlab("year")+
    ylab(str_wrap("peak water level (m)", width=10))+
    #geom_smooth(method=lm, aes(x=yr, y=max.first.value), se=F)+
    theme(axis.text.x = element_text( size=12),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
          legend.title = element_blank(),
          legend.text=element_text(size=14),
          legend.position="bottom")+
    scale_colour_manual(labels = function(x) str_wrap(x, width = 30),
                         values=c("#000000", "#ABA9A9") )+
    labs(colour="WSC Station Name")+xlim(2000,2020)
  #### Combine plots 2 and 3 into one plot 
  p23 = p2/p3+plot_layout(guides="collect")
  #### Combine the p2/3 plot with the map
  gif.plot = p1+p23+plot_layout(widths=c(4, 4)) & 
    theme(legend.position="bottom", plot.title = element_text(size=30,face="bold", hjust=0.5)) &
    plot_annotation(title=paste("June ",year.main))
  #### Save the gif to your file
  ggsave(plot=gif.plot,filename= paste0("year", year.main, ".png") ,width=13.8, height=9.5, units = "in") 
}


### List all the files in the gif and combine them into a stacked image
files=list.files(pattern="*.png")
images <- map(files, image_read)
images <- image_join(images)
### Animate the stacked image
gif = image_animate(images, fps = 1, dispose = "previous")

## save as a gif
setwd(images.wd)
image_write(gif, paste0("gif_", stage4.date, ".gif"))


# plot number of observations for each lake in each month
## Plot of number of june observations as a map
all.classified %>% 
  mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
  as_tibble() %>% dplyr::filter(month==6)%>% dplyr::select(-geometry)%>% 
  group_by(OBJECTID, month) %>%count() %>% ungroup() %>% 
  left_join(lakes.sf, by="OBJECTID") %>% st_as_sf() %>% 
  st_transform(crs = crs.plot) %>% 
  ggplot()+
    geom_sf(aes(fill=n), color=NA)+
    theme_bw()+
    annotation_scale(text_cex = 0.9)+
    geom_sf(data=mack.basin, color="grey65")+
    scale_fill_viridis_c(begin=0, end=1, direction=1, limits=c(0,22))+
    theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
          legend.title = element_text(size=14, face="bold"),
          legend.text=element_text(size=14),
          legend.position="bottom")+labs(fill="Number of Years of June Data")+
  guides(fill=guide_colourbar(direction="horizontal",  barwidth = 20, title.position = "top", title.hjust = 0.5))

## Plot number of observations in each month as a bar chart
month.labs = c("April", "May","June", "July", "August", "September")
names(month.labs)=c(4,5,6,7,8,9)
all.classified %>% 
  mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
  as_tibble() %>% dplyr::select(-geometry)%>% 
  group_by(OBJECTID, month) %>%count() %>% 
  ggplot()+geom_histogram(aes(x=n), binwidth=1, fill="#EFA292", color="grey60")+
  facet_wrap(~month, nrow=4, labeller = labeller(month=month.labs))+
  theme_bw()+
  xlab("number of years of data")+ylab("number of lakes")+
  theme(axis.text.x = element_text(size=12),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(size=12),
          legend.title = element_text(size=14, face="bold"),
          legend.text=element_text(size=14),
        strip.text = element_text(size=12, face="bold"))
```



# Analyze spatial patterns in connectivity
```{r}
# Distance from edge of island
## Import islands
setwd(pil.wd)
mack.islands = st_read("MackenzieDeltaIslands.shp") %>% 
  dplyr::select(fid, geometry) %>% 
  st_transform("EPSG:32608")
mack.islands$area = units::set_units(st_area(mack.islands), km^2)
## Import lakes
setwd(valFile.path)
lake.sf = st_read(valFileName , stringsAsFactors = F) %>%  
  rename(OBJECTID=ID) %>% dplyr::select(OBJECTID, geometry) %>% 
  st_transform("EPSG:32608")

## Join lakes to the island that they are in. 
islands.join = lake.sf %>% st_join(mack.islands) %>% filter(!is.na(fid))

## Import classification results & group by month
setwd(int.wd)
results.summary = read_rds(paste0("final.class_", stage3.date, ".Rdata")) %>% 
  dplyr::select(.pred_class, OBJECTID, year, month) %>% 
  group_by(OBJECTID, month)%>% 
  summarise(class.median = median(as.numeric(as.character(.pred_class))),
            count = n()) %>% filter(count>=10) %>% ungroup() 

## fraction of lakes in each island  that are high connectivity for each month (>1.5 class)
island.grouped = results.summary %>% 
  left_join(islands.join, by="OBJECTID") %>% filter(!is.na(fid)) %>% 
  group_by(fid, month, area) %>% 
  summarise(num.lakes = n(),
            lakes_1 = sum(class.median>=1.5),
            pct_1 = lakes_1/num.lakes) %>% 
  ungroup() %>% filter(num.lakes>=10) %>% 
  mutate(area.km = as.numeric(area))

## Plot fraction if lakes in each island that are high connectivity (>1.5)--not much of a pattern w/ area.
ggplot()+geom_point(data=island.grouped, aes(x=area.km, y= pct_1))+facet_wrap(~month, nrow=2)  
## spatial pattern? See June pattern--probably related to discharge timing
island.grouped %>% 
  left_join(mack.islands %>% dplyr::select(fid, geometry), by="fid") %>% 
  st_as_sf() %>% 
  ggplot()+
  geom_sf(aes(fill=pct_1))+
  theme_bw()+ scale_fill_viridis_b() + 
  facet_wrap(~month)

# Look at connectivity via latitude 
setwd(int.wd)
## Import data
lake.centroid = lakes.sf %>% st_centroid()
results = read_rds(paste0("final.class_", stage3.date, ".Rdata")) %>%
  dplyr::select(.pred_class, OBJECTID, year, month) %>% 
  mutate(.pred_class = as.numeric(as.character(.pred_class))) %>% 
  left_join(lake.centroid %>% dplyr::select(OBJECTID, geometry),
            by="OBJECTID") %>% st_as_sf()
results$lat = as_tibble(st_coordinates(results))$Y

## group data by latitude
results.lat = results %>% 
  mutate(lat.group = case_when(
    lat < 67.75 ~ "< 67.75",
    lat>=67.75 & lat < 68 ~ "67.75-68.00",
    lat>=68 & lat <68.25 ~ "68.00-68.25",
    lat>=68.25 & lat < 68.5 ~"68.25-68.50",
    lat>=68.5 & lat < 68.75 ~"68.50-68.75",
    lat>=68.75 & lat < 69 ~"68.75-69.00",
    lat>=69.00 & lat < 69.25 ~ "69.00-69.25",
    lat>=69.25 & lat < 69.5 ~ "69.25-69.50",
    lat>=69.5 ~ "> 69.50"
  )) %>% as_tibble() %>% select(-geometry, -lat) %>% 
  group_by(lat.group, year, month) %>% 
  summarise(count=n(),
            class.mean = mean(.pred_class),
            class.median=median(.pred_class)) %>% ungroup()
## Count what fraction of lakes are observed in each latitude band in each month, only plot >75% of lakes
results.lat.sum = results.lat %>% group_by(lat.group,month) %>% 
  summarise(max.count = max(count)) %>% ungroup() %>% 
  right_join(results.lat, by=c("lat.group", "month")) %>% 
  mutate(pct = count/max.count) %>%  filter(pct>0.75)
## Group the data further into year groups for easier plotting
results.year.sum = results.lat.sum %>% group_by(year, month) %>% count() %>% filter(n>=8) %>% 
  right_join(results.lat, by=c("year", "month")) %>% filter(!is.na(n)) %>% 
  mutate(year.group = case_when(
    year<=2001 ~ "2000-2001",
    year>2001 & year <= 2003 ~ "2002-2003",
    year>2003 & year <=2005 ~ "2004-2005",
    year>2005 & year <=2007 ~ "2006-2007",
    year>2007 & year <=2009 ~ "2008-2009",
    year>2009 & year <=2011 ~ "2010-2011",
    year>2011 & year <=2013 ~ "2012-2013",
    year>2013 & year <=2015 ~ "2014-2015",
    year>2015 & year <=2017 ~ "2016-2017",
    year>2017 & year <=2019 ~ "2018-2019",
    year>2019 & year <=2021 ~ "2020-2021",
    
  ))
results.year.sum$lat.group = factor(results.year.sum$lat.group, levels = c("< 67.75", "67.75-68.00", "68.00-68.25", "68.25-68.50", "68.50-68.75",
                                                        "68.75-69.00", "69.00-69.25", "69.25-69.50", "> 69.50"))


## Actually run the plot
results.year.sum %>% ggplot()+
  geom_line(aes(x=lat.group, y = class.mean, color=year, group=year))+
  geom_point(aes(x=lat.group, y = class.mean, color=year))+
  scale_color_viridis_c()+
  theme_bw()+
  theme(
    axis.text.x=element_text(angle=45, hjust=1)
  )+
  facet_grid(rows=vars(year.group), cols=vars(month))+coord_flip()


# Analyze trend over time
## Get ids of lakes with at least 10 years of classifications in june
good.ids = results %>% as_tibble() %>% dplyr::filter(month==6)%>%  select(-geometry)%>% group_by(OBJECTID, month) %>%count() %>% 
  filter(n>10)
good.ids = good.ids$OBJECTID

## filter to june classification data
nested.data = results %>% as_tibble() %>% select(-geometry, -lat) %>% dplyr:: filter(month==6) %>% 
  dplyr::filter(OBJECTID %in% good.ids) %>% 
  group_by(OBJECTID) %>% nest()
## for each lake, calculate the trend (tau) and pvalue
row.combo=NULL
for (i in 1:length(nested.data$OBJECTID)){
  dat.june = nested.data$data[[i]] %>% arrange(year)
  OBJECTID = nested.data$OBJECTID[[i]]
  test.obj=MannKendall(dat.june$.pred_class)
  S=test.obj$S[[1]]
  tau = test.obj$tau
  pval = test.obj$sl
  col.combo = cbind.data.frame(OBJECTID, pval, S, tau)
  row.combo=rbind.data.frame(row.combo, col.combo)
}
## Format trend data results
trend.data=row.combo %>% as_tibble()%>% 
  mutate(trend = case_when(
    tau>0 & pval < 0.05 ~ "increasing connectivity trend",
    tau<0 & pval < 0.05~ "decreasing connectivity trend",
    pval>0.05 ~ "no monotonic trend")) %>% 
  left_join(lakes.sf, by="OBJECTID") %>% st_as_sf() %>% 
   st_transform(crs = crs.plot)
## Select colors for trend plot  
cols.trend = c("increasing connectivity trend" = "red", "decreasing connectivity trend" = "blue", "no monotonic trend"="grey85")
## Create trend map
trend.plot =ggplot(data=trend.data)+
  geom_sf(aes(fill=trend), color=NA)+

  scale_fill_manual(values=cols.trend)+
  theme_bw()+
  annotation_scale(text_cex = 0.9)+
  geom_sf(data=mack.basin.large, color="grey65")+
    geom_sf(data=combo.location, aes(color=rivr), size=5)+
    scale_colour_manual(guide="none",     values=c("#000000", "#ABA9A9"))+
  theme(axis.text.x = element_text(angle=45, hjust=1, size=12),
        #strip.text = element_text(size=14, face="bold"),
        title = element_text(size=14, face="bold"),
        axis.text.y = element_text(size=12),
        legend.title = element_blank(),
        legend.text=element_text(size=14),
        legend.position = "bottom",
        legend.justification = "center")+labs(fill="Trend")+ggtitle("June Connectivity Trend")+
  guides(fill=guide_legend(nrow=3, byrow=TRUE, legend.justification="center"))
## Create plot of date of peak water level over time
dis.doy= combo.df %>% as_tibble() %>% ggplot()+geom_line(aes(x=yr, y=max.first.doy, color=rivr))+
    geom_point(aes(x=yr, y=max.first.doy,color=rivr), size=1.5)+theme_bw()+
    geom_point(data=combo.df %>% filter(year==year.main), aes(x=year, y=max.first.doy, color=rivr), size=5)+
    ylab(str_wrap("date of peak water level", width=15))+
    theme(axis.text.x = element_text(size=12),
          #strip.text = element_text(size=14, face="bold"),
          title = element_text(size=14, face="bold"),
          axis.text.y = element_text(angle=45, hjust=1, vjust=-0.05,size=12),
          axis.title = element_text(size=14, face="bold"),
          legend.title = element_blank(),
          legend.text=element_text(size=14),
          legend.position = "bottom")+
    scale_colour_manual(labels = function(x) str_wrap(x, width = 30),
                         values=c("#000000", "#ABA9A9") )+
    scale_y_continuous(breaks = c(121,127,135, 144,152, 160), labels = c("May 1st", "May 7th","May 15th",
                                                                         "May 24th", 
                                                                     "June 1st", "June 9th"))+
    labs(colour="WSC Station Name")+xlim(2000,2020)+xlab("year")

## Combine the trend map with the discharge plot
trend.plot + (dis.doy/plot_spacer()) +plot_layout() 
  


trend.data %>% as_tibble() %>% dplyr::select(-geometry) %>% group_by(trend) %>% count()


```























######################### OLD STUFF #########################


















# old
```{r}
# Import GECI validation data for training/testing
setwd(valFile.path)
lake.class = st_read(valFileName , stringsAsFactors = F) %>% as_tibble() %>% 
  rename(OBJECTID=ID)
lake.class %>% group_by(type) %>% count()
# group classes by functional connectivity
lake.class.grouped=lake.class %>% mutate(final_class = case_when(
  type == "g1" | type== "g7" | type== "g4" ~ "1", #always low functional
  type== "g3_5" ~ "2", #some at high dis, none at low dis
  type == "g3"  | type =="g6" ~ "3", #high at high dis, none at low dis
  type == "g2_5" ~ "4", #high at high dis, some at low dis
  type == "g2" | type=="g5" |type=="g2_add"~ "5", # high at both low and high dis
  type== "badImage" | type == "coastal" | type =="notLake" | type=="uncertain" ~ "remove",
  type=="mediumThenHigh" | type=="moderateBoth" | type=="lowThenMedium"~ "not enough data")) %>% select(-geometry, -type)
lake.class.grouped %>% group_by(final_class) %>% count()

# Now, use Boruta, see below, to figure out which variables are important
#https://www.datacamp.com/community/tutorials/feature-selection-R-boruta
set.seed(90)
trainData <- prep %>% left_join(lake.class.grouped, by="OBJECTID") %>% 
  dplyr::filter(!is.na(final_class) & (final_class != "remove") & (final_class !="not enough data")) %>% 
  mutate(classes=as.factor(final_class)) %>% dplyr::select(-final_class) %>%  
  dplyr::select(-OBJECTID) %>% drop_na()
## remove unnecessary columns using Boruta
attach(trainData)
colnames(trainData)
## apply the boruta
boruta.train <- Boruta(classes~., data = trainData,
                       mcAdj = TRUE, pValue = 0.01,doTrace = 2,ntree = 50)
# print the results of the boruta
print(boruta.train)
boruta.train$finalDecision
# plot the results of the boruta
plot(boruta.train, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 
## Get rid of tentative (yellow on the plot)
boruta.bank=TentativeRoughFix(boruta.train)
plot(boruta.bank, colCode=c("lightgreen", "yellow", "red", "blue"),
     cex.axis=.7, las=2, xlab="",sort=TRUE, main="") 

#Select the variables to use during the classification
important.cols=getSelectedAttributes(boruta.bank, withTentative = F) 
bank_df <- attStats(boruta.bank)
print(bank_df) #visualize variable importance


###Create a decision tree! Tutorial here: https://www.gmudatamining.com/lesson-13-r-tutorial.html
set.seed(123)
geci.val =  prep %>% left_join(lake.class.grouped, by="OBJECTID") %>% 
  dplyr::filter(!is.na(final_class) & (final_class != "remove") & (final_class !="not enough data")) %>% 
  mutate(classes=as.factor(final_class)) %>% 
  dplyr::select(-final_class) %>%  
  dplyr::select(c("OBJECTID","classes", all_of(important.cols))) %>% drop_na()


#look at the data to make sure it is as expected
skimr::skim(geci.val)
# split data into test and train
set.seed(234)
geci.split = initial_split(geci.val, strata=classes)
geci.train = training(geci.split)
geci.test = testing(geci.split)

# Prepare data for later 5-fold cross validation
geci.folds5 = vfold_cv(geci.train, v=5, strata = classes)
# get the recipe setup for pre-processing
geci.recipe = recipe(classes~., data=geci.train) %>% 
   update_role(OBJECTID, new_role = "OBJECTID")  ###########################Maybe exclude the ID instead of updating the role, then adding it later
geci.recipe %>% prep() %>% bake(new_data=geci.train)#print the training data
# Set up the model and what parameters we want to tune
tree_model = decision_tree(cost_complexity=tune(),
                           tree_depth=tune(),
                           min_n=tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
# setup the workflow with both the model and the preprocessing recipe
tree_workflow=workflow() %>% 
  add_model(tree_model) %>% 
  add_recipe(geci.recipe)
# Hyperparameter tuning
set.seed(345)
##Build a grid of parameter options
tree_grid = grid_regular(cost_complexity(), 
                         tree_depth(), 
                         min_n(), levels=5)
#tune the grid
tree_tuning = tree_workflow %>% 
  tune_grid(resamples = geci.folds5, grid = tree_grid)
## Show the top 5 best models based on roc_auc metric
tree_tuning %>% collect_metrics() %>% 
  ggplot()+geom_point(aes(x=min_n, y=mean, color=as.factor(tree_depth), size=cost_complexity), alpha=0.3)+facet_wrap(~.metric)
##select the best model based on accuracy (it is the same model whether we pick using accuracy or roc_auc)
best_tree = tree_tuning %>% 
  select_best(metric="accuracy")
best_tree

# Finalize workflow with the new tuned parameters
final_tree_workflow = tree_workflow %>% finalize_workflow(best_tree)
# Fit the model to the training dataset
set.seed(456)
tree_wf_fit = final_tree_workflow %>% fit(data=geci.train)
tree_fit = tree_wf_fit %>% extract_fit_parsnip()#show the  model
# expore the model
library(vip)
library(rpart.plot)
vip(tree_fit) #variable importance
tree.plot =rpart.plot::rpart.plot(tree_fit$fit, roundint=FALSE, type=5, extra=2)
tree.plot
# setwd(figures.filePath)
# ggsave(tree.plot, width=6.5, height=3.14, filename=tree.figure.name)
# apply the model to the test set
tree_last_fit = final_tree_workflow %>% last_fit(geci.split)
tree_last_fit %>% collect_metrics()
tree_predictions=tree_last_fit %>% collect_predictions()
conf_mat(tree_predictions, truth = classes, estimate = .pred_class)
confusionMatrix(tree_predictions$.pred_class, tree_predictions$classes)

```

## same thing but for random forrest
```{r}
# pre-process
geci.train= geci.train %>% drop_na()
geci.test = geci.test %>% drop_na()
geci.rec =recipe(classes ~., data=geci.train) %>% 
  update_role(OBJECTID, new_role = "OBJECTID") 
geci.pre=prep(geci.rec)
geci.juiced = juice(geci.pre)
geci.juiced %>% count(classes)
#Make model specifications & get ready to tune
tune.spec = rand_forest(
  mtry=tune(), #when you are making leaves of the tree, how many do you sample at each split--all predictors or just a few
  trees = 500,
  min_n=tune()# How long do you keep splitting. How many datapoints have to be in a node before you stop splitting
  ) %>% 
  set_mode("classification") %>% set_engine("ranger") #ranger is just one way of doing random forest
tune.wf=workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(tune.spec)
# Train hyperparameters with 5-fold cross validation
set.seed(345)
geci.fold  = vfold_cv(geci.train, v=5, strata=classes)
## tune parameters
#doParallel::registerDoParallel()
tune.res=tune_grid(
  tune.wf,
  resamples=geci.fold,
  grid=20
)
## take a look at parameters
tune.res %>% collect_metrics() #look at all the metrics
tune.res %>% select_best("roc_auc") #select best accuracy
tune.res %>% 
  collect_metrics() %>% 
  filter(.metric=="roc_auc") %>% 
  select(mean, min_n, mtry) %>% 
  pivot_longer(min_n:mtry,
               values_to="value",
               names_to="parameter") %>% 
  ggplot(aes(value, mean, color=parameter))+
  geom_point(show.legend="FALSE")+
  facet_wrap(~parameter, scales="free_x")
# Tune again using info from prior tuning
set.seed(456)
rf.grid= grid_regular(
  mtry(range=c(5,20)),
  min_n(range=c(0,10)),
  levels = 5
)
regular.res=tune_grid(
  tune.wf,
  resamples=geci.fold,
  grid=rf.grid
)
##roc_auc
regular.res %>% 
  collect_metrics() %>% 
  filter(.metric=="roc_auc") %>% 
  mutate(min_n=factor(min_n)) %>% 
  ggplot(aes(mtry, mean, color=min_n))+geom_point()+geom_line(alpha=0.5, size=1.5)
##accuracy
regular.res %>% 
  collect_metrics() %>% 
  filter(.metric=="accuracy") %>% 
  mutate(min_n=factor(min_n)) %>% 
  ggplot(aes(mtry, mean, color=min_n))+geom_point()+geom_line(alpha=0.5, size=1.5)
# select the best option
best.acc =select_best(regular.res, "roc_auc")
final.rf=finalize_model(
  tune.spec,
  best.acc
)
# Check out variable importance for the model as a whole
library(vip)
final.rf %>% set_engine("ranger", importance="permutation") %>% 
  fit(classes~.,
      data = juice(geci.pre) %>% select(-OBJECTID)) %>% 
  vip(geom="point")
# see how the model does on the testing data
final.wf = workflow() %>% 
  add_recipe(geci.rec) %>% 
  add_model(final.rf)
final.res=final.wf %>% last_fit(geci.split)
final.res %>% collect_metrics()
final.res %>% collect_predictions() %>% 
  mutate(correct=case_when(classes==.pred_class~"Correct", TRUE ~"Incorrect")) %>% 
  bind_cols(geci.test) %>% 
  ggplot(aes(x=med_Nsmi_ratio_m,y=med_Gb_ratio_m, color=correct))+
  geom_point(size=3, alpha=0.4)+labs(color=NULL)+scale_color_manual(values=c("gray80", "darkred"))+theme_bw()
final.pred.rf= final.res %>% collect_predictions()

setwd("C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/intermediaryDownloads")
write_rds(final.pred.rf, "predictions_traintest_20220826.Rdata")


conf_mat(final.pred.rf, truth = classes, estimate = .pred_class)
table=confusionMatrix(final.pred.rf$.pred_class, final.pred.rf$classes)

table

## plot the training/testing dataset on a map
### This is the fitted model to use on the other datasets
fitted.wf.rf= pluck(final.res, 6)[[1]]
train.ids = geci.train$OBJECTID
test.ids = geci.test$OBJECTID
rf.class = cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>% select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>% 
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test")) %>% 
  gather(key="class",value="group", -OBJECTID, -split ) %>% 
  left_join(lakes.sf %>% 
              select(-Shape_Leng, -Shape_Area, -count), 
            by="OBJECTID") %>% 
  st_as_sf()

ggplot(rf.class)+geom_bar(aes(x=group, fill=class), 
                          stat="count", position="dodge") +
  facet_wrap(~split)+theme_bw()


rf.class.raw = cbind(predict(fitted.wf.rf, geci.val), geci.val) %>% as_tibble() %>% select(OBJECTID, .pred_class, classes)%>% rename(.obs_class = classes) %>% 
  mutate(split=case_when(
    OBJECTID %in% train.ids~"train", 
    OBJECTID %in% test.ids~"test")) %>% 
  left_join(lakes.sf %>% 
              select(-Shape_Leng, -Shape_Area, -count), 
            by="OBJECTID") %>% 
  st_as_sf()


rf.class.raw %>% filter(split=="test") %>% filter(.pred_class=="4") %>% mapview(zcol=".obs_class")



library(ggalluvial)
alluvial.prep=rf.class.raw %>% filter(split=="test") %>% as_tibble() %>% select(-geometry)%>% group_by(.pred_class, .obs_class) %>% summarise(freq=n())

ggplot(data=alluvial.prep, aes(axis1=.obs_class, axis2 = .pred_class, y=freq))+
  geom_alluvium(aes(fill=.pred_class))+geom_stratum(aes(fill=.pred_class))+
  geom_text(stat = "stratum",
            aes(label = after_stat(stratum))) +
  scale_x_discrete(limits = c("Survey", "Response"),
                   expand = c(0.15, 0.05)) +
  theme_void()




# apply classification to all lakes 
all.data= channels.lakes %>% as_tibble() %>% #filter(OBJECTID %in% goodLakes.val.ids) %>% 
  #filter(year %in%  highest.maxDis.yrs) %>% 
  # mutate(year.group = case_when(
  #   year>=2000 & year <=2001 ~ "2000-2001",
  #   year>=2002 & year <=2003 ~ "2002-2003",
  #   year>=2004 & year <=2005 ~ "2004-2005",
  #   year>=2006 & year <=2007 ~ "2006-2007",
  #   year>=2008 & year <=2009 ~ "2008-2009",
  #   year>=2010 & year <=2011 ~ "2010-2011",
  #   year>=2012 & year <=2013 ~ "2012-2013",
  #   year>=2014 & year <=2015 ~ "2014-2015",
  #   year>=2016 & year <=2017 ~ "2016-2017",
  #   year>=2018 & year <=2019 ~ "2018-2019",
  #   year>=2020 ~ "2020",
  # )) %>% 
  mutate(year.group = case_when(
    year>=2000 & year<=2003 ~ "2000-2003",
    year>=2004 & year<=2007 ~ "2004-2007",
    year>=2008 & year<=2011 ~ "2008-2011",
    year>=2012 & year<=2015 ~ "2012-2015",
    year>=2016 & year<=2019 ~ "2016-2019",
    year>=2020 & year<=2023 ~ "2020-2013",
  )) %>% 
  dplyr::group_by(OBJECTID, year.group) %>% 
  dplyr::summarise(med_dw_rat_m = median(dom_wv_ratio_m),
            sdev_dw_rat_m = sd(dom_wv_ratio_m),
            kurt_dw_rat_m = kurtosis(dom_wv_ratio_m),
           med_R_ratio_m = median(R_ratio_m),
           sdev_R_ratio_m = sd(R_ratio_m),
           kurt_R_ratio_m = kurtosis(R_ratio_m),
           med_B_ratio_m = median(R_ratio_m),
           sdev_B_ratio_m = sd(R_ratio_m),
           kurt_B_ratio_m = kurtosis(R_ratio_m),
           med_G_ratio_m = median(R_ratio_m),
           sdev_G_ratio_m = sd(R_ratio_m),
           kurt_G_ratio_m = kurtosis(R_ratio_m),
           med_Gb_ratio_m = median(Gb_ratio_m),
           sdev_Gb_ratio_m = sd(Gb_ratio_m),
           kurt_Gb_ratio_m = kurtosis(Gb_ratio_m),
           med_Ndssi_ratio_m = median(Ndssi_ratio_m),
           sdev_Ndssi_ratio_m = sd(Ndssi_ratio_m),
           kurt_Ndssi_ratio_m = kurtosis(Ndssi_ratio_m),
           med_Nsmi_ratio_m = median(Nsmi_ratio_m),
           sdev_Nsmi_ratio_m = sd(Nsmi_ratio_m),
           kurt_Nsmi_ratio_m = kurtosis(Nsmi_ratio_m),
           count=n()) %>% ungroup() %>% filter(count>=10) %>% select(-count)



# see how the model does on the testing data
mod=final.wf %>% fit(rbind(geci.train, geci.test))
all.pred = predict(mod, all.data)

all.res = cbind.data.frame(all.data, all.pred) %>% select(OBJECTID, year.group, .pred_class) %>% 
  as_tibble() %>% left_join(lakes.sf, by="OBJECTID") %>% st_as_sf()

ggplot(data=all.res, aes(fill=.pred_class))+geom_sf(color=NA)+facet_wrap(~year.group)

```


```{r}
setwd(valFile.path)
lake.structure = st_read(valFileName , stringsAsFactors = F) %>% as_tibble() %>% 
  rename(OBJECTID=ID) %>% 
  mutate(
    structure.class = case_when(
      type %in% c("g5", "g6", "g7") ~ "no channel",
      type %in% c("g2","g2_5", "g3", "g3_5", "g4", 
                  "moderateBoth", "lowThenMedium", "mediumThenHigh", "g2_add") ~
        "channel"
    )
  ) %>% 
  dplyr::filter(!is.na(structure.class) ) %>% 
  dplyr::select(-type)





```




```{r}
#Classify using the other method...
prep %>% ggplot()+geom_density(aes(x=med_Nsmi_ratio_m))
classifications=prep %>% 
  mutate(connectivity=ifelse(med_Nsmi_ratio_m>=0.98 &  
                                 sdev_Gb_ratio_m<0.24, 
                             "high functional connectivity", "low functional connectivity"))


# do the "through time" classification
connectivity.groups = classifications%>% dplyr::select(ID, time_period, connectivity) %>% 
  spread(time_period, connectivity)


# get lakes that are always high functional connectivity
always.high.ids=connectivity.groups %>% 
  filter(`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")
always.high.ids=always.high.ids$ID

#get lakes that are always low functional connectivity
always.low.ids = connectivity.groups %>% 
  filter(`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
always.low.ids=always.low.ids$ID

# get lakes that go from high to low connectivity over time
high.to.low.ids = connectivity.groups %>% 
  filter((`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity") |
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
          (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
           )
high.to.low.ids=high.to.low.ids$ID

# get lakes that go from low to high connectivity over time
low.to.high.ids = connectivity.groups %>% 
  filter((`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity") |
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity"))
low.to.high.ids=low.to.high.ids$ID

# get lakes that flip back and forth connectivity through time
flip.ids = connectivity.groups %>% 
  filter((`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity") |
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
          (`2000-2004`=="high functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
         (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="low functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="high functional connectivity" & 
           `2015-2019`=="low functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="high functional connectivity")|
           (`2000-2004`=="low functional connectivity" & 
           `2005-2009`=="high functional connectivity" & 
           `2010-2014`=="low functional connectivity" & 
           `2015-2019`=="low functional connectivity")
         )
flip.ids = flip.ids$ID

# classify connectivity variability through time
final.lakes=connectivity.groups %>% mutate(
  group = case_when(
    ID %in% always.high.ids ~ "always high functional connectivity",
    ID %in% always.low.ids ~ "always low functional connectivity",
    ID %in% high.to.low.ids ~ "high to low functional connectivity over time",
    ID %in% low.to.high.ids ~ "low to high functional connectivity over time",
    ID %in% flip.ids ~ "connectivity switches back and forth through time"
  )
) %>% filter(!is.na(group)) %>%
  mutate(group2 = ifelse(ID %in%flip.ids| 
                           ID %in% low.to.high.ids |
                           ID %in% high.to.low.ids, 
                         "variable functional connectivity", group))  

# import shapefile
setwd("C:/Users/whyana/OneDrive - University of North Carolina at Chapel Hill/DocumentsLaptop/001_ Graduate School/Research/Connectivity/Mackenzie/Data/shapeFiles")
mack.lakes.sf = st_read("mackenzieGoodLakes.shp") %>% rename(ID=OBJECTID)
set.seed(100)
sample.mack.lakes = mack.lakes.sf %>% sample_n(length(mack.lakes.sf$ID)*0.2) %>% 
  mutate(fxd_ndx=row_number())
st_write(sample.mack.lakes, "mackenzieLakes20pct.shp")

#join shapefile to results
results.sf = final.lakes %>% left_join(mack.lakes.sf, by="ID") %>% st_as_sf()
mapview(results.sf, zcol="group")
```

